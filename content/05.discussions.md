## Discussions

In this work, we presented a new open-source Python package *MMV_Im2Im* package for image-to-image transformations in bioimaging applications. We demonstrated the applicability on more than ten different problems or datasets to give biomedical researchers a holistic view of the general image-to-image transformation concepts with diverse examples. This package was not a simple collection of existing methods. Instead, we distilled the knowledge from existing methods and created this generic version with state-of-the-art ML engineering techniques, which made the package easy to understand, easy to use, and easy to extend for future. We hope this package can serve the starting point for other researchers doing AI-based image-to-image transformation research, and eventually build a large shared community in the field of image-to-image transformation for bioimaging.

### Further works

One of main directions for extending *MMV_Im2Im* is to pre-pack common bioimaging datasets as a Dataset module, so that DL researchers can use it for algorithm development and benchmarking, and new users can easily use it for learning microscopy image-to-image transformation. We will continue improving the functionalities of the package, such as supporting more models and methods, such as diffusion based models [@doi:10.48550/arXiv.2208.14125], unsupervised denoising [@HDN] or Imaginaire (<https://github.com/NVlabs/imaginaire>). Besides, we also plan to develop two auxiliary packages *MMV_Im2Im_Auto* and *MMV_Im2Im_Active*. In specific, when you have a reasonable amount of training data, *MMV_Im2Im_Auto* will take advantage of the fact that MMV_Im2Im is fully configurable with yaml files, and automatically generate a set of potentially good configurations, then find the optimal solution for you by cross validation. On the other hand, when you only have very limited training data, or even with only pseudo ground truth, *MMV_Im2Im_Active* will help to build preliminary models from the limited training data, and gradually refine the model with human-in-the-loop by active learning [@doi:10.1101/491035]. All the packages will also be wrapped into napari plugins [@doi:10.5281/zenodo.3555620] to allow no-code operation and therefore be more friendly to users without experience in programming.

The image-to-image transformation frameworks implemented in the current version do not explicitly take temporal information into account. We treat images (2D or 3D) at each time step independently. Thanks to the flexibility of aicsimageio, our package can directly read even multi-channel 3D timelapse data (i.e, 5D) during training or inference, if necessary. But the computation is done at individual time steps. A common method to integrate the temporal context with spatial information is the convolutional recurrent neural network (CRNN) [@doi:10.1109/TPAMI.2020.2992393]. The support of CRNN will be part of our future work.

Another type of microscopy image analysis problem related to image-to-image transformation is image registration, where we learn how to transform the “floating” image spatially so that it is optimally aligned with the reference image in the physical space. Recent methods are able to transform the floating image into its registered version through deep neural networks [@doi:10.1109/CVPR42600.2020.00470]. This will be another important direction for future extension.

Beyond *MMV_Im2Im*, we hope to develop a similar package for other problems (without re-inventing wheels). For example, as we mentioned in the instance segmentation application, Mask-RCNN type models are also very powerful instance segmentation methods and, in theory, can also be generalized beyond 2D images. However, Mask-RCNN would fit more to a detection framework, instead of image-to-image transformation. It will be supported in our *MMV_NDet* (NDet = N-dimensional detection) package, currently under development.

## Code availability and requirements

* Project name: MMV_Im2Im (Microscopy Machine Vision, Image-to-Image transformation)

* Project home page: https://github.com/mmv-lab/mmv_im2im

* Operating system(s): Linux and Windows (when using GPU), also MacOS (when only using CPU)

* Programming language: Python

* Other requirements: PyTorch 2.0.1 or higher, PyTorch Lightning > 2.0.0, and all other additional dependencies are specified as in https://github.com/MMV-Lab/mmv_im2im/blob/main/setup.py

* License: MIT license

To enhance the accessibility and traceability of our toolbox, we registered it with biotools (bio.tools ID: biotools:mmv_im2im) and workflow hub (??REF?? (https://doi.org/10.48546/workflowhub.workflow.626.1).). 

## Data and model availability

In general, all data used in this work were from open-accessible public repositories, released with other publications under open-source licenses. All data used in this work were only for research purposes, and we confirm that we didn’t use these for any other non-commercial purpose or commercial purpose. The scripts we used to download and re-organize the data can be found in our release repository: https://github.com/MMV-Lab/mmv_im2im/tree/main/paper_configs/prepare_data. Detailed information about each dataset is listed below, in the same order as the Results section.

**1. Labelfree prediction of nuclear structure from 2D/3D brightfield images:**

**2D:** The data were downloaded from <https://zenodo.org/record/6139958#.Y78QJKrMLtU> and <https://zenodo.org/record/6140064#.Y78YeqrMLtU>. We used all the data from the two sources, while 15% of the data were held-out for testing. In specific, for data source 1 (<https://zenodo.org/record/6139958#.Y78QJKrMLtU>), it contains a timelapse tiff of 240 time steps, each with 5 channels (only channel 3 and 5 were used in this work).

* Channel 1 : Low Contrast Digital Phase Contrast (DPC)
* Channel 2 : High Contrast DPC
* Channel 3 : Brightfield (the input in our study)
* Channel 4 : EGFP-α-tubulin
* Channel 5 : mCherry-H2B (the ground truth in our study)

For data source 2 (<https://zenodo.org/record/6140064#.Y78YeqrMLtU>), it contains two sub-folders (train and test), each with snapshots sliced from different time lapse data. Each snapshot is saved as six different tiff files (only the _bf and the second channel of _fluo were used in this work):

* _bf: bright field (the input in our study),
* _cyto: cytoplasm segmentation mask
* _dpc: phase contrast
* _fluo: two channel, first cytoplasm, second H2B (the H2B channel is the ground truth in our study)
* _nuclei: nuclei segmentation mask
* _sqrdpc: square-root phase contrast

**3D:** The data were downloaded from the hiPSC single cell image dataset from the Allen Cell Quilt Bucket: <https://open.quiltdata.com/b/allencell/packages/aics/hipsc_single_cell_image_dataset>, which was released with the publication [@doi:10.1038/s41586-022-05563-7]. Each field-of-view (FOV) is a multi-channel 3D image, of which the brightfield and the corresponding structure channels were used as input and ground truth, respectively. Experiments were done on four different cell lines: fibrillarin (structure_name = “FBL”), nucleophosmin (structure_name = “NPM1”), lamin b1 (structure_name = “LMNB1”), and histone H2B (structure_name = “HIST1H2BJ”), with 20% of the data were held-out for testing.
 
 **2. 2D semantic segmentation of tissues from H&E images**

The data were downloaded from <https://warwick.ac.uk/fac/cross_fac/tia/data/glascontest/>, which was originally used for MICCAI Glas challenge [@doi:10.1109/TMI.2015.2433900;@doi:10.1016/j.media.2016.08.008]. There were one training set (85 images) and two test sets (60 and 20 images). We kept the same train/test split as in the challenge.


**3. Instance segmentation in microscopy images**

**2D:** The data were downloaded from <https://bbbc.broadinstitute.org/BBBC010> for segmenting C. elegans from widefield images [@doi:10.1021/cb900084v]. We used all images from the dataset, while 5% of the data were held-out for testing.


**3D:** The data were downloaded from the hiPSC single cell image dataset from the Allen Cell Quilt Bucket: <https://open.quiltdata.com/b/allencell/packages/aics/hipsc_single_cell_image_dataset>. We used the lamin b1 cell line (structure_name = “LMNB1”) for these experiments. Each raw field-of-view (FOV) is a multi-channel 3D image (DNA dye channel, membrane dye channel, structure channel and brightfield channel), with the instance segmentation of all nuclei available. In our two experiments, we used the DNA dye channel and the brightfield channel as input, respectively, while using the same 3D instance segmentation ground truth. 20% of the data were held-out for testing.

**4. Comparing semantic segmentation and instance segmentation of organelles from 3D confocal microscopy images**

The data were downloaded from the hiPSC single cell image dataset from the Allen Cell Quilt Bucket: <https://open.quiltdata.com/b/allencell/packages/aics/hipsc_single_cell_image_dataset>. We used the fibrillarin cell line (structure_name = “FBL”) for these experiments. Each raw field-of-view (FOV) is a multi-channel 3D image (DNA dye channel, membrane dye channel, structure channel and brightfield channel). The input is always the structure channel. Then, we used the FBL_fine workflow in the Allen Cell and Structure Segmenter [@doi:10.1101/491035] to generate the semantic segmentation ground truth, and we used the cell instance segmentation to group fibrillarin segmentations belonging to the same cell as unique instances (see more details in Results section), which will be used as the instance segmentation ground truth. The FBL_fine segmentation workflow was optimized for this cell line, which can be considered as a good approximation of the real truth. To be conservative, we excluded images where the mean intensity of the structure channel is outside the range of [450, 500], so that the results from the FBL_fine workflow can be a better approximation of the real truth. After removing the “outlier” images, we held-out 20% of the data for testing.

**5. Unsupervised semantic segmentation of intracellular structures from 2D/3D confocal microscopy images**

**2D:**  The data were downloaded from the hiPSC single cell image dataset from the Allen Cell Quilt Bucket: <https://open.quiltdata.com/b/allencell/packages/aics/hipsc_single_cell_image_dataset>. We used the tight junction cell line (structure_name = “TJP1”) for this experiment. The original image and corresponding structure segmentation were both in 3D. We took the max intensity projection of the raw structure channel and the corresponding structure segmentation for experimenting unsupervised 2D segmentation. The correspondence between images and segmentations were shuffled to simulate unpaired ground truth. 20% of the data were held-out for testing.


**3D:** The data were also downloaded from the hiPSC single cell image dataset from the Allen Cell Quilt Bucket: <https://open.quiltdata.com/b/allencell/packages/aics/hipsc_single_cell_image_dataset>. We used three different cell lines for these experiments: Golgi (structure_name = “ST6GAL1”), mitochondria (structure_name = “TOMM20”), and histone H2B (structure_name = “HIST12BJ”). For Golgi and mitochondria, we simply used the corresponding structure segmentation from the dataset. For histone H2B, we took the released nuclear instance segmentation and converted it to binary as semantic segmentation results. The correspondence between images and segmentations were shuffled to simulate unpaired ground truth. 20% of the data were held-out for testing.


**6. Generating synthetic microscopy images from binary Masks**

**2D:**  The data were downloaded from the hiPSC single cell image dataset from the Allen Cell Quilt Bucket: <https://open.quiltdata.com/b/allencell/packages/aics/hipsc_single_cell_image_dataset>. We used the nucleophosmin cell line (structure_name = “NPM1”) for this experiment. The original image and corresponding structure segmentation were both in 3D. We took the max intensity projection of the raw structure channel and the corresponding structure segmentation for this experiment. The input is binary segmentation, while the ground truth is the raw image.


**3D:** The data were downloaded from the hiPSC single cell image dataset from the Allen Cell Quilt Bucket: <https://open.quiltdata.com/b/allencell/packages/aics/hipsc_single_cell_image_dataset>. We used the histone H2B cell line (structure_name = “HIST1H2BJ”) for these experiments. For the experiment with coarse masks, we used the binarized nuclear segmentation as the input, while for the experiment with detailed masks, we used the structure segmentation of H2B as the input. The ground truth is always the raw 3D structure image.


**7. Image denoising for microscopy images**

The data were downloaded from <https://publications.mpi-cbg.de/publications-sites/7207/>, which was released with the publication [@doi:10.1038/s41592-018-0216-7]. We used two datasets “Denoising_Planaria.tar.gz” and “Denoising_Tribolium.tar.gz”. We kept the original train/test splitting in the datasets.

**8. Imaging modality transformation from 3D confocal microscopy images to stimulated emission depletion (STED) microscopy images**

The data were downloaded from <https://zenodo.org/record/4624364#.Y9bWOoHMIqJ>, which was released with the publication [@doi:10.1038/s41592-021-01155-x]. We used two datasets *Microtubule* and *Nuclear_Pore_complex* from “Confocal_2_STED.zip”. We kept the original train/test splitting in the datasets.

**9. Staining transformation in multiplex experiments**

This dataset were downloaded from <https://zenodo.org/record/4751737#.Y9gbv4HMLVZ>, which was released with the publication [@doi:10.1038/s42256-022-00471-x]. We used the dataset “BC-DeepLIIF_Training_Set.zip” and “BC-DeepLIIF_Validation_Set.zip”. In our three experiments, we always used the IHC image as the input, and used standard hematoxylin stain image, mpIF nuclear image and mpIF LAP2beta image as ground truth, correspondingly.

**10. Models and sample data**

To help researchers get started with our tool, we have deposited all models used in the manuscript as well as sample data at https://doi.org/10.5281/zenodo.10034416.

## Abbreviations


## Conflict of interest

The authors report no conflict of interest.

## Funding

## Authors' contributions


## Acknowledgments

We would like to thank the MONAI team for their support in our process of development, and the aicsimageio team for advice on how to integrate aicsimageio into the package. This work is supported by the Federal Ministry of Education and Research (Bundesministerium für Bildung und Forschung, BMBF) under the funding reference 161L0272, and by the Ministry of Culture and Science of the State of North Rhine-Westphalia  (Ministerium für Kultur und Wissenschaft des Landes Nordrhein-Westfalen, MKW NRW).